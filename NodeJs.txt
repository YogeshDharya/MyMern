A dynamically typed language which means we don't need to specify the data types of the variables being used IT LEARNS DYNAMICALLY ON THE GO 
Node uses an event driven , Non blocking model which makes it lightweight for handeling request at the server with low latency(Go lang even lower)
A run time ENVIRONMENT (REPL) that allows JS to run on a server . It allows us to run JS on the server unlike before when it could be executed only on the
It helps us running JS outside a browser within a run-time(which works as a JS engine) provided by node that helps us in executing JS code 
at the server-side or the back-end browser . 
It works this way , when we hit a URL(from the browser) which points to our server , we can use node to handle that request 
by means of a file that exists in the server's file system then we can produce a response 2 the client from this js file   
we can use Read Eval Print Loop for running node within an environment which could interpret JS input and produce an output in an interactive manner 
However , since we want our node 2 exist within a file , we would  
CODE WITH HARRY https://www.youtube.com/watch?v=BLl32FvcdVM 5:73
WE CAN'T USE IMPORT WITHIN VANILLA NODE  SINCE NODE DOESN'T UNDERSTAND ES6
http is an inbuilt module which allows node to send data over the http protocol createServer method which is required to imported via require() 
const server=httpVar.createServer((req,res)=>{
    res.writeHead(200,{'Content-Type':'text/html'})//contentType specifies the data-type it will be required to parsed into 
    //writeHead is for RESPONSE HEADERS such as C evenmore below 
    res.writeHead(200,{"location":"localhost:8081/user/todos})
    fs.readFile('index.html',(error,data)=>{//fs.fileRead(index.html,utf8,(data,error)=>{
        if(error){
            res.writeHead(404)
use response.writeHead() before .write(JSON.stringify()) since it defines write's nature 
            res.write('Error:file not found')
        }else{
            res.write(data)//ALWAYS KEEP A RETURN STATEMENT within a JS function within node 
            res.end("Success")//without this the request would be like an infinite loop since the backend wouldn't respond because below 
            //tells the browser that the response is complete however u'll find it below too if the error occurs here ASSIGN 
            //if its kept at the top the backend will stop sending the response then and their however the rest of the code will still execute BUT
        //when accompanied with a return statement u'll C the JS behaviour CAUSE IT IS WHAT IT IS , JS (just runnin on the backend cause of node)
        }//also Success msg can be sent as response.write("Sucess")
})
//for custom files we would need a node's file System module called fs look below
    //just res.write() 4 plain text however .writeHead() for html
    res.end()//to mark the end of the 
})//you can put the following .listen() here itself or use a variable for storing the result produced from createServer 
var assignPort = server.listen(portVar,(error)=>{//An IIF just like the above one 
 if(error){//also the server hereby GETS BINDED TO THE GIVEN PORT NO.
        console.log("something insn't correct")
    }else{
    console.log(`Nodejs server started on port ${port}`)
}
}) //after u've required via require(index.html) use
When u want to compute the data passed from the front-end's user request url , we can do so by utilizing it in chunks via the 
NodeJs request event emiter which takes two parameters varUsed.on("event",(callBack for handling ex - error , chunks end)=>{})
One should actually process the request data in chunks while checking for the data ending and push it to some variable and later parse it into JSON
if (method === "POST") {
    let body = '';
    request.on('error', (err) => {
        console.error(err);
    }).on('data', (chunk) => {
        body += chunk;
    }).on('end', () => {
        body = JSON.parse(body);
        todoList.push(body.name);
    });
} 
package.json stores configuration information of  ur node project 
it contians the following dependencies (also called packages installed via npm):
EVEN REACT EXISTS WITHIN the package.json file 
production dependencies    - listed under dependencies key and gets installed in the server when project is deployed 
development dependencies - key "devDependencies" 
file specifies the dependencies required for this node project along with the basic details such as license author scripts
scripts - allows us to have various actions while performing building testing deploying activities , via automation?!
such as npm run / test / eject etc 
for example within script : {"start" : node index.js}// then npm start would trigger the index.js file BUT if u are willing to perform
js manipulation WITHOUT ENDING THE SERVER session in that case npm install nodemon --save-dev and keep dev key with value 
nodemon fileName.js within script SERVER STAYS ALIVE WATCH MODE   
4TH JAN 2023 8:14 PM we can now execute js files within a server cause of the run-time provided by node just started(at port 8081) fast 
life / baller song on spotify 
pnpm-lock.json : keeps a track of the history of packages present or is like a version tracking file of the package.json file  
MODULE IS THE GLOBAL OBJECT FOR NODE 
POSTMAN OR THUNDER CLIENT  (FE): a utility tool which provides a testing platform for our backend by mimicing the front end 
Express is a framework which helps in affirming the nature/type of the response a backend sends to the frontend(ThunderClient)
We can perform querry selecting on request urls plus use routes on them as well its as easy as app.get("/urlSection",(req,res)=>{//something res.end("sessionEnded")})
GOOD thing : now it would actually b like the backend is sending response to the frontend via res.send(instead of res.write()) within app.get()
4 json -> res.json()//STILL DON'T FORGET RETURN ALTHOUGH RES.END
the index.js file decides what route should be taken and consequently which route file should be taken PLUS the route file decides which  corresponding controller function should be invoked 
controllers :are functions which help in  segregating the control/bussiness logic from the routing logic PLUS reusability becomes possible 
AS SUCH IT seperates the function which handles the route request from the code which actually processes this request , ie. controller() body doesn't exist within the index.js file instead within 
some ___.controller.js file
req is an instance/obj of IncomingMessage 
FORGOT HERE 
req also has properties such as url and method which UPON DESTRUCTURING can be used for checking presence of specific paths within the 
api request like this const {url , method} =request => if(url === "users")
and later method can be used 4 checking if the protocol is if(method === "GET/POST/DELETE whatever pleases u")  
req.querry : this section of the req url may or may not have successive params that is to say we do not define the route in the params. ' . MAY BE THAT'S WHY WE DON'T NEED req.params 
cause we explicitly define the possible outcomes'' consequences 
req.params : used in case of dynamic routes , that is to say we explicitly pass in some information AS parameters in the form of key value of paris seperated by & and assigned by = beginning from ? 
unlike static routes which remian the same at all times HOWEVER ordering has to be taken care of because when the latter is kept before the dynamic route ; ONLY in that case it will be considered as 
a static route , otherwise when a static route is kept is after a dynamic route then even the static one might be considered as the url's parameter making it a dynamic route 
like this 
app.get("users/:uuid",getTheUserId);
app.get("users/search",getAllInfor); //although static , search would be interpreted as the dynamic portion 
. ' .keep static routes before the dynamic ones otherwise things wouldn't work    
Look for reasons for existence
EXPRESS js is a minimal ? and flexible ? ROUTING AND MIDDLEWARE  node JS framework  provides a robust set of features for app development unlike Node which facilitates developers  
express routes and exress controllers and express iteself 
EXpress by default adds 200 and 404 status codes if a route matches or doesn't match 
Route validation : we need to ensure that there are ways to test the incoming front-end's request apis in order to avoid polluting our DB with un-wanted data thereby reducing the compute time required by processing only 
the correct request 
While performing route validation we may have to type a lot code while considering the possiblities . ' . in order to avoid our files from looking clumsy we can use a package called joi which helps us in doing the same 
stuff in an organized schema testing manner where we define a schema for our data plus detain a set of valid values for our schema which can be used for testing purpose 
Joi validation :
(i) defining the schema
(ii) using the data.validate(schema) for testing 
EXPRESS MIDDLEWARE : - kept logging the req's method ,url, timeStamp and data properties 2 like starting, ending& pending date WAS LOOKING 4 A WAY OUT 2 AVOID REPEATING the exact same code way way more than a couple of times 
 app.use(at the top of the file)  serves as the global middleware which gets invoked at the time of initialization and can be used 4 including other use-defined  middlewares 
that is to say app.get() has these parameters 2 which are req,res and next but since we hadn't called the nxt parameter like this next() it behaved like a get request listener (middlewares are () which are processed C BELOW) 
middleware get invoked in the middle of the time when a server receives a request and sends a request .
//NEW ABOVE AND below 
Middleware functions in Nodejs are special functions with access to the request and response objects.
They can do a particular task like logging some value to the console, validating a request.
They can choose to
pass the handle to the next middleware function in the current request-response lifecycle using the next() method, or
end the current request-response lifecycle
Middleware functions can be applied at different levels like
Application level - the middleware function set directly to the express app object
Router level - invoke the middleware only for a particular express router
middleware functions have ACCESS to all three that is - req obj , res obj and the next middleware () in the apps request - response cycle 
in order to pass the req and res objects to the next middleware () it is necessarry 4 each middleware () 2 accept the CALLBACK referred to as next 
IT NEED NOT RETURN TRUE OR FALSE , cause when something fails , it should return the control back to the user
const captureDateMiddleware = (req, res , next)=>{
  console.log(`
  Method : ${req.method},
  url : ${req.url}//NOTICE no localhost:portNo. since this has to b GENERAL 
  timeStamp : ${new Date()}
  `),next()
}
How did you enable "captureDateMiddleware" to be used for all requests to the express application? *
We can pass a middleware function to be invoked at the application level by the app.use() method, where app denotes the Express app object.
https://expressjs.com/en/guide/writing-middleware.html BELOW difference between middleware usage in app.use() and router.use()
router.use() mounts middleware for the requests served by the specific router
app.use() mounts middleware for all requests to the app
https://stackoverflow.com/a/27228010
// NEW 
app.all("todos/",(req,res)=>{//for listening to all requests 4 some specific route GET,POST&DELETE which remain unmatched 
ALSO
app.all("*",(req,res)=>{})//for all routes other than the ones explicitly defined
we use app.use() method for using a validator () upon importing in the main index.js file ALSO we can use it at router files 2
the prior procedure works by keeping the app.use(validator()) on top of the app.use(routers) statments cause of the waterfall rule , it'll go and process the other routes only if the api request has the required headers   
NOTES 2 RECALL :-
app.use() : both 4 routers as well validator() 
127. something refers to the localhost
dot env:ALSO this helps in keeping sensitive data hidden process is a global object in Node.js and can be used from any module without requiring it. It can be used for purposes like reading environment variables using the process.env object, for example, process.env.port. Read more
https://www.freecodecamp.org/news/node-process-object-explained/
 when we don't want our system to search in the device's OS level region when server looks for process.env file 
therefore we create a .env file which helps the system to search in the local PLUS env files help in preparing our project for running in more than just our local environment . Here we mention those environment specific 
configurations which when fulfilled according to the corresponding environment ; will make the project to run on the concerned environment error free , these can be third party software usage authorizations , api-keys passwords etc 
WHEN U C .env.example while working with an opensource project one should copy the contents and keep it in a seperate .env file where u mention ur own environment/local system's configuration details for running this new project 
Node.js is a backend JavaScript runtime environment to to execute JS code without a browser

You can use it to build to web server that listens for client requests and responds to it

Node’s built-in http module’s createServer() method can be used to create an HTTP server

The createServer() method accepts a callback function with two parameters, request and response

request denotes the incoming request object

response denotes the response object which will be send back to the client

The callback function gets executed every time a new request is received

The request object has properties like

method - type of HTTP request method

url - path of the resource requested for

headers - request headers

You can use these values to conditionally respond to the requests based on the request properties like HTTP method, path or headers

The response object methods can be used to set the response

To set a response status code and headers, use the writeHead() method exposed by the response object. Eg: response.writeHead(200, { 'Content-Type': 'text/html' });

write() method accepts the text to be sent as response body

end() method tells the server the request has been completed

To bind the HTTP server created using createServer() to a specific port, use the listen() method

It also accepts a callback function which gets executed only once when the server is started
To read data sent as request body, use the request object’s data and end events

data event is triggered if there’s data remaining in the stream to be read

end event is triggered if the request data stream is empty

NPM is a repository for JavaScript packages.

You can use npm install to download your project dependencies from the NPM registry

The package.json file can be used to specify your project dependencies using its dependencies field

Express.js is a Node framework for creating web applications and APIs

You create an Express application handler by const express = require(‘express’); const app = express()

It provides methods for each of the HTTP request method types like

app.get() for GET request

app.post() for POST request

app.all() is a special method that listens to all HTTP request types to a particular path

To specify the callback for a particular route, let’s say GET request to /user, you do app.get("/user", callbackFunction)

Using Express’s inbuilt json middleware, we can directly parse the JSON request body as a JavaScript object.

To do that, add app.use(express.json()) before any routes

You can use for example, request.body.jsonProperty1 to fetch a property in your request body, "jsonProperty1"

The response object’s methods to use are

status() - to set the response status code

send() - send response body data

end() - trigger end of response

Find the

Solution code here
Solutions to the Curious Cats questions here MUST Do https://docs.google.com/document/d/1Pxj6s7EeTI_L4ctZ2LQaylwObfUFMLT_IMoXlnIx4QA/edit
Further Reading
Why should you use NodeJS https://www.freecodecamp.org/news/what-are-the-advantages-of-node-js/
More sophisticated TODO app using Express https://developer.okta.com/blog/2018/11/27/first-router-node-express
Newfound Superpower
Knowledge of the Node.js backend framework
Know-how of the Express.js library
Now you ca
Create simple applications using the Node.js framework
Use Node.js to create a web server that listens to multiple routes
Utilize the Express.js library in your Node applications

EXPRESS ROUTERS
because if there multiple sub-routes originating from the same parent route (path??) we can have a separate file for handling all the requests corresponding to the same parent route "v1/" to v1/v2 v1/v3 
ROUTER->MIDDLEWARE->CONTROLLER 
const express = require("express")
const app=express() // 
const router = express.Router() // now router is an obj/instance of the Router class of Express || Express' Router Class 
this router object has similar methods as the app object had for handling the various http request methods such as 
router.get("/v1/todos",(req,res)=>{})
                                M               O               N               G               O           DB
It is schema-free and does away with the need to define a fixed structure. You can just start adding data and it isn’t necessary to have any relation between those. It uses dynamic schema&inherently  The only restriction with this is the supported data structures.
The document format used in place of rows and columns can be easily mapped by programming languages
Horizontally scalable i.e, data can be distributed across multiple servers to improve performance whereas for relational databases, performance can only be improved by increasing the performance of the server it’s run on.
) 
Provides good support for Hierarchical data like JSON or XML. For large amounts of unstructured data, SQL based DB would be significantly slower.
Offers higher efficiency and reliability which in turn helps with better storage capacity and speed requirements.

STUDY THIS AS WELL U C THE UPPER CASE  

MongoDB uses dynamic schema and inherently supports hierarchical data (nested data) as it’s a NoSQL database. This means that the schema (structure) is dynamically created from the data and individual data entries don't have to be in the same format. Hence, there’s no overhead to adding complex data like we saw above, making it a preferred choice to store this kind of data.


Other advantages of using MongoDB
As MongoDB can be scaled horizontally, MongoDB clusters can be geographically distributed on servers to improve query performance.

MongoDB is a persistent (or disk-based) database unlike Redis, which is a NoSQL based in-memory database. Though Redis can be significantly faster than MongoDB when the stored data is smaller in size, MongoDB can operate much faster as the database gets larger.

Sharding is the concept of storing partitions of data (shard) in different database servers.
For sharding in relational databases like MySQL, application level changes are required as well as needing to suffer downtime due to interlinking of the tables. These also require servers with high specifications which are costly.
Due to the horizontal scalability, sharding in MongoDB is cost-effective as buying several low-cost machines is often cheaper than buying a smaller number of machines with significantly higher specifications.


Additionally, MongoDB stores data in the BSON (Binary 1 and 0 JSON) format which is space efficient 
MongoDB takes a lesser amount of storage space to store data compared to MySQL if there is a lot of variety in the structure of every document saved
MongoDB takes a higher amount of storage space to store data compared to MySQL if the documents are all similar in structure. This is because using a static schema and normalization reduces storage for MySQL
HOWEVER AT TIMES BSON TAKES MORE SPACE THAN JSON http://bsonspec.org/faq.html
BSON basically is an improvised version of the popular JSON format used
BSON supports additional data types like Date out of the box in addition to the data types like String, Boolean, Number, Array supported by JSON.
Parsing data is much quicker with BSON as data isn’t saved in text form (human-readable) as in JSON. BSON adds metadata to like lengths of string used to make queries faster

As popular programming languages like JavaScript, Python has data types (JavaScript Object, Python Dictionary) to which JSON can be directly mapped, it removes any transformation required to process the data.

However, in the below cases, Relational databases are a better bet than NoSQL databases
If large amount of structured data is to be written and it’s critical to avoid loss of data
If the structure of data may not change, "fixed schema" of relational databases can be better even with the overhead of designing the schema as it becomes possible to perform complex queries using SQL
If storage is an issue and need to avoid data redundance

NoSQL databases allow for more flexibility (dynamic schema) and are suitable for data with complex relationships, like hierarchical.

MongoDB stores data records as documents (specifically BSON documents) which are gathered together in collections. A database stores one or more collections of documents.

Mongo shell commands can be used to create Databases and Collections as well as to perform CRUD operations on the Documents

Find the

Solutions to the Curious Cats questions https://docs.google.com/document/d/1ReNIWcyuPsRdId668q5FAQJrcU2F7C901t9f98qrLdc/edit?usp=sharing

MONGOSH
SQL vs NoSQL//MONGO DB is schema-less . ' . we may get to find un-related documents within collections due to restriction-less db.collection.insertOne({}) operation causing problems while processing them from Node  
ISSUE 
PLUS data in MongoDB is BSON which is required to be converted to JSON for querry processing that OBJECT DATA MODELLING is req
table : group of rows and colums while 
collection : group of documents or a simple record 
document is equivalent to one ROW/record in SQL  
PUT  send the entire document for the NoSQL DB UNLIKE PATCH where 
Commands(No ;)
show dbs(only the dbs with collections)
use dbName //BY DEFAULT control is within test db
db ->shows the current db name 
db.createCollection("FirstRecord/Collection")
db.getCollectionNames()//has a different name
dbName>db.collectionName.insertOne({key0:"value" ,key1:null})
//this means a document has been created within collectionName
also insert(single) ?? insertMany([]) and bulkWrite(ObjId)//multi-purpose cmd CRUD 
//pretty NEW 
dbName>db.collectionName.find({}).pretty()//Empty {} 4 all dox whereas
dbName>db.collectionName.find({"key":"value"})// 4 specific results
dbName>db.collectionName.find({}).limit(1).pretty() // for having just a single document
dbName> db.collectionName.drop() drops all documents 
dbName->db.dropDatabase()//removes every data from DB ??
dbName->db.collectionName.drop() // drops collection else returns false 
db.COLLECTION_NAME.update({SELECTION_CRITERIA}, {$set:{UPDATED_DATA}}, {
     upsert: <boolean>,
     multi: <boolean>,
     writeConcern: <document>,
     collation: <document>,
     arrayFilters: [ <filterdocument1>, ... ],
     hint:  <document|string>        
   })

Parameters:

The first parameter is the Older value in the form of Documents. Documents are a structure created of file and value pairs, similar to JSON objects.
The second parameter must contain a $set keyword to update the following specify document value.
The third parameter is optional.
Optional Parameters:

Upsert: The default value of this parameter is false. When it is true it will make a new document in the collection when no document matches the given condition in the update method.
multi: The default value of this parameter is false. When it is true the update method update all the documents that meet the query condition. Otherwise, it will update only one document.
writeConcern: It is only used when you do not want to use the default write concern. The type of this parameter is a document.
Collation: It specifies the use of the collation for operations. It allows users to specify the language-specific rules for string comparison like rules for lettercase and accent marks. The type of this parameter is a document.
arrayFilters: It is an array of filter documents that indicates which array elements to modify for an update operation on an array field. The type of this parameter is an array.
hint: It is a document or field that specifies the index to use to support the filter. It can take an index specification document or the index name string and if you specify an index that does not exist, then it will give an error.
REMOVAL 
consider taking the object id while insertOne as the best option for uniquely identifying a document's property 
BECAUSE when multiple elements satisfy the querrying condition ALL THE ELEMENTS GET DELETED 
also MongoDB _id value has is 96 bit number(96 bit = 8bytes  Don't consider database names when generating _id value. The whole _id value isn’t randomly generated but only a part of it has a counter which is initialized from a random value.
) which has the current system being used in Unix epochs 
db.reservations.remove({ "_id": ObjectId("4f4e3fcf380d499d856e728d")})//
MULTIPLE removal conditions required to b mentioned ?
db.reservations.remove({ $and : [ {"city" : "London"}, {"starts_at" : "2020-06-18T20:13:58"} ] })
NOTICE the commas between the two ARRAY !!!! elements which are json objects 
db.reservations.count({"key":"value"}) //more conditions use callBacks 
db.reservations.count({$where : ()=>{return new Date(this.starts_at)>= new Date()}}) // Date(inputString) --> 2 date
db.reservations.find({"city" : { $in: ["Delhi", "Bangalore"] }})//

MONGOOSE
// WHAT if document contains data similar to some existing doc(Say id)&& (some property is missing) NO PROBLEM AT ALL ! 
OBJECT DATA MAPPER 
ODM vs ORM https://dev.to/anudeepreddy/what-is-the-difference-between-an-odm-and-an-orm-2e7a
Mongoose is the preferred NodeJS based  ODM library for MongoDB which helps in mapping an object's keys? to a db's collection's dox' prop 
They act as the translator between your programming language and a document-based database. It encapsulates all the complexities involved in the mapping and provides wrapper methods for us developers 
to utilize.
it serves  as a utility on top of MongoDB which provides schema defintion and validation properties for MongoDB PLUS helps in transforming mongo's BSON data format to JSON format
It also helps in performing CRUD and querying activities where we do so on Mongoose and Mongoose does it on MongoDB internally 
Since MongoDB has been installed as a survive , when we start our OS , it runs on the port  27017 . ' . we can define the db_uri for establishing a connection between our server and the DB mongodb://127.0.0.1:27017/dbName
mongoose
    .connect(db_uri)//mongodb://127.0.0.1:27017/dbName
    .then(()=>console.log("Connected to MongoDB") app.listen(port,()=>{console.log("NEW NEW listening on",port)}))
    .catch((error)=>console.log("Encountered this error : ",error))
a schema defines the DOCUMENT  properties of a given collection within a db by CORRESPONDING IT TO the OBJECTS ' KEYS PLUS 
a Mongoose Model is a wrapper on the Mongoose schema which provides  an interface to the DB for CRUD , querying etc ,
we ensure that we define TYPES to these keys . ' . it becomes possible 2 validate the data entered by the user at the front end  which are defined within models folder  named as dbName.model.js
we can keep default values plus we can even make keys required or optional 
const mongoose = require("mongoose");
const modelSchema = new mongoose.Schema(//READ READ 2 define a schema for the Mongo documents
    {
        title:String,
        author : [String],//{type : [authorSchema]},
        content : String,
        publishedAt: Date
    }
const newModel = mongoose.model("collectionName",modelSchema);// Mongoose wrapper over the schema object to apply methods over a collection of a Mongo database
module.exports=newModel
in the index.js file we'll be using another route which upon being hit would move to the blog.controller from the blog.router 
NOTE : we need 2 ensure that we aren't hardCoding the MODEL's SCHEMA's OBJ's KEY's (doc's props) instead we should be utilizing the data sent from the front-end
which has 2 b a request with a req.body which is seen in case of a POST req 40:13 BUT data coming from the front-end in the request url isn't in the json format. ' . one needs to parse it into json first by using 
a MIDDLEWARE app.use(express.json(NoParams and right below app=express() )) BUT we need to ensure that this data is WRITTEN into the DB as well for which .save method is used from the mongoose library
const newRecord = async (req,res)=>{
 const record = new requiredBlog(req.body);
 const result = await record.save(newRecord);//FORGOT data in the remember to keep await before model methods 
 res.status(200).json(result);//FORGOT that the response type has to be according to the express method being used 
}
MONGOOSE theory :
we keep underscores before v this way _v so that mongoose can keep track of versions here's why :-
https://kb.objectrocket.com/mongo-db/understanding-the-mongoose-__v-field-1011
//BUT BUT BUT u didn't specify that Mongoose needs to check on this collection how is that happening 
we can avoid the deprecation warning in Mongoose via the following commands (ALL TOGETHER|| 1 IS ENOUGH)??
useCreateIndex: true
useNewUrlParser: true
useUnifiedTopology: true
useFindAndModify: false
// 
CRUD with ExpressJS & MongooSE 

we will be required to define a schema utilizing the mongoose.Schema() method and later will have to assign it to a blogModel as mongoose.model("dbName??",schemaJustCreated) export it 
then we'll get to C that this instance of mongoose.model() class has member methods such as .find({}) 
Do's & Don'ts of Blogging 
speak with audience narrating stories general store active speaching 
client server https://www.youtube.com/watch?v=6YgsqXlUoTM
https://dev.to/t/explainlikeimfive
https://linkedin.com/in/alok722

What if we are asked , we take away packages from u ?
the way we get 2  C in MongoDB later we can use the response object to send the result stored in the variable 
BUT BUT BUT 
we need to keep this function of async type since we need to AWAIT until we fetch all the data from DB OVER ALL IS AS follows 
const todos = required("./modelsNOTblog/someMongooseBlog");
router.get("v1/2do/",async(req,res)=>{//still a route
//IMPORTANT INFORMATION RIGHT BELOW 
console.log(`
URL : "v1/2do/${req.url == "/" ? "":req.url 
HTTP-Method : ${req.method}
timeSTamp = ${new Date()} 
`)
const result = {
    name : req.body.name,//keys in accordance with the document format
    startDate : req.body.start ,
    endDate : req.body.end
}
const result = await Todos.find({},(error,allTodos)=>{
    if(error){
        console.log(error)//not throw error since it ins't try-catch
        res.status(500).send()//send is kept blank for sending just the type of status code which is 500 
    }else{
        res.send(allTodos)
    }
}res.send(result))//question not resolved  
})//BUT BUT BUT u didn't specify that Mongoose needs to check on this collection how is that happening 
HOWEVER , if u are willing to WRITE into ur MongoDB 
THEORY                                                HTTP RESPONSE STATUS CODES & MORE
https://developer.mozilla.org/en-US/docs/Web/HTTP/Status
200    
201     Created
204     No Content
400    Bad Request
404    Not Found
500     Internal Error
https://dev.to/remi/express-req-params-req-query-and-req-body-4lpc
req.params contains route parameters (in the path portion of the URL)
req.query contains the URL query parameters (after the ? in the URL)
req.body contains key-value pairs of data submitted in the request body
https://www.mscharhag.com/api-design/http-post-put-patch
POST requests to create new resources in a server
PUT requests to update the whole of an existing resource
PATCH requests to update parts of an existing resource.
CURL command practice is required

GET requests shouldn't contain body because http get method SHOULD just contain a REPRESENTATION of the resource it is attempting to GET from the server . ' . it SHOULD just possess reference 4 that data  
not the data itself , however if we are willing to fetch all the data from the DB  whose duration  falls in the range of time specified from the front-end then should it be POST request ? Since we will be required to pass this
range of time as data in the body of the request which is possible only in the case of a POST request since GET requests don't possess req.body 
responses have body         yes
safe                            yes
idempotent          yes 
Cache-able
This is the place where params come into picture . These params are present within the url's path itself after a "?" instead of the req.body thing 
like this : https://www.youtube.com/watch?v=Tr8EU1kvV_o
router.get("/", async (req, res) => {
  console.log(//NOTICE no request body 
    `URL:  /v1/todos${req.url == "/" ? "" : req.url}, Method:  ${req.method}, Timestamp: ${new Date()}`
  );
  if (req.query.startDateMax && req.query.startDateMin) {//we could have destructured it ?Try this in replit 
    let startDateMax = new Date(req.query.startDateMax);
    startDateMax.setTime(startDateMax.getTime());
    let startDateMin = new Date(req.query.startDateMin);
    startDateMin.setTime(startDateMin.getTime());
    Todos.find(
      {
        startDate: {
          $lte: startDateMax,//$lte stands for <= these are MongoDB operators others include $set and https://www.tutorialsjar.com/mongodb-operators-tutorial/
          $gte: startDateMin,//$gte stands for >=
        },
      },
      (err, allTodos) => {
        if (err) {
          console.log(err);
        } else {
          res.send(allTodos);
        }
      }
    );
  } else {
    Todos.find({}, (err, allTodos) => {
      if (err) {
        console.log(err);
        res.status(500).send();
      } else {
        res.send(allTodos);
      }
    });
  }
});
//IMPORTANT INFORMATION

Why do we have mongoose if we have MySQL 
 We want to have little bit  interdependency : Complete restriction less data insertion might cause erros in the long run 
  scaling .
also v1 stands for the current version so that if some upgradation causes v2 to occur then instead of making changes to the entire existiing packages   
.stringify() into string then !!variable into boolean
router.post("/",async (req,res)=>{//USE middlewares instead since they have access to req obj 2 other than res and next which passes the control to  
  console.log(`
  URL : "v1/todos/${req.url =="/"? "":req.url} ,
  method : ${req.method} , 
  timeStamp : ${new Date()},
  `)
  console.log("request body ",req.body)//GET reqs don't possess bodies since it is bad practise 
  const newTodo ={
    name : req.body.name,
    startDate : req.body.startDate,
    endDate : req.body.endDate
  }
  const result = await Todos.create(newTodo,(error)=>{
    if(error){
      console.log(error);
      res.status(500).send();
    }else{
      console.log("Newly Created todo item",newTodo);
      res.status(201).send(newTodo)
    }
  });
})

===
const mongoose=required("Mongoose");
const newSchema=mongoose.schema({
"name":string,
"email":SomePackage
})
const newModel = new mongoose.model("db'sCOLLECTION",newSchema)
module.exports=newModel;

const express=require("express");
const app=express();
const model = required("./models/newModel")
app.use("someUrl/req.url",async (req,res)=>{
const newRecord = {
"name": req.body.name,
"email":req.body.email
const result = await neRecord.save((error,result)=>{if(error){console.log(error)}else{console.log(result)}});//REMEMBER we perform access .save() from the newRecord var itself which stores data from the front-end 
res.status(200).json(result);
}
//create(),.find(),.findByIdAndUpdate() . findByIdAndDelete() vs .save(why on newlY created item instead of the todos)
router.put("users/path",(req,res)=>{
    console.log(`
    url : users/auth/${req.url == "/" ? "":req.url},
    method : ${req.method},
    timeStamp : ${new Date()},
    body:${req.body},
    _id:${req.body.}
    `)
    const requiredId = req.body._id;
    const updatedTodo = {
        name : req.body.name,
        startDate : req.body.startDate,
        pending : req.body.pending,
        endDate : req.body,endDate
    } 
    Todos.findByIdAndUpdate(requiredId,updatedTodo ,(err,doc)=>{
        if(err){//kept short as err but C 2 lines below
            console.log(err)
            res.status(500).send()
        }else if(doc == null){
            res.status(400).send({error : "Resource not found"})
        }else{
            res.status(204).send()
        }
    })
})
request.delete("users/:_id",async (req,res)=>{
    const reqId = req.params._id;//REMEMBER
    console.log(`
    body : ${req.body}
    startDate : ${req.}
    endDate : ${req.body.endDate}
    pending : ${req.body.pending}
    url : ${req.url}//REMEMBER typed it as req.body.url
    method : ${req.method}//made the above mistake twice
    `)
    Todos.findByIdAndDelete(reqId, (err,result)=>{
        if(err){
            console.log(err)
            res.status(400).send()
        }else if(result == null){
            res.status(500).send({error: "Resource not Found"})
        }else{
            res.status(204).send()//had result instead of doc plus was of no use 
        }
    })
})

password authentication 
DB stores encrypted password , bcrypt decrypts while using .compare()
session1 until payload expiry the way we see in .dev session2 
header(encrypted)payload(enc)signature(hashed of before 2)===token 
WHEN DONE WITH MONGODB : https://github.com/PacktPublishing/MongoDB-Essentials---A-Complete-MongoDB-Guide
